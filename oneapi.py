# -*- coding: utf-8 -*-
"""oneAPI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_drUZk3-Iwz8BFpiA8JOFWcvuCbxCi9W
"""


import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
import gensim
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

# Load dataset
df = pd.read_csv("./training_data.csv")

# Remove unnecessary columns
df = df.drop(['Source', 'PublishDate'], axis=1)

# Remove missing values
df = df.dropna()

# Define stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Define function for text preprocessing
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans("", "", string.punctuation))
    
    # Tokenize text
    tokens = nltk.word_tokenize(text)
    
    # Remove stopwords
    tokens = [token for token in tokens if token not in stop_words]
    
    # Lemmatize tokens
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    # Join tokens to form text again
    text = " ".join(tokens)
    
    return text

# Preprocess text in the dataset
df['clean_title'] = df['Title'].apply(preprocess_text)

df['clean_headline'] = df['Headline'].apply(preprocess_text)


# Convert preprocessed text to gensim dictionary and corpus
texts = [doc.split() for doc in df['clean_title']]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Define number of topics and run LDA model
num_topics = 4
lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)

# Print top words in each topic
for topic in lda_model.show_topics(num_topics=num_topics):
    print(topic)

import gensim

# Convert preprocessed text to gensim dictionary and corpus
texts = [doc.split() for doc in df['clean_headline']]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Define number of topics and run LDA model
num_topics = 5
lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)

# Print top words in each topic
for topic in lda_model.show_topics(num_topics=num_topics):
    print(topic)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVR
from sklearn.metrics import classification_report

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['clean_title'], df['SentimentTitle'], test_size=0.2, random_state=42)

# Vectorize text using TF-IDF
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print(X_train_vec.shape)
print(X_test_vec.shape)
x_title = X_train

print(y_train.value_counts())

# Train SVM classifier
svmTitle = LinearSVR()

svmTitle.fit(X_train_vec, y_train)

# Predict on test data
y_pred = svmTitle.predict(X_test_vec)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['clean_headline'], df['SentimentHeadline'], test_size=0.2, random_state=42)

# Vectorize text using TF-IDF
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)
print(X_train_vec.shape)
print(X_test_vec.shape)
x_Headline = X_train

# Train SVM classifier
svmHeadline = LinearSVR()
svmHeadline.fit(X_train_vec, y_train)

# Predict on test data
y_pred = svmHeadline.predict(X_test_vec)

# Load dataset
dfTest = pd.read_csv("./test_data.csv")

dfTest['clean_title'] = dfTest['Title'].apply(preprocess_text)
dfTest['clean_headline'] = dfTest['Headline'].apply(preprocess_text)

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(x_title)
X_test_vec = vectorizer.transform(dfTest['clean_title'])
print(X_train_vec.shape)
print(X_test_vec.shape)
y_predTitle = svmTitle.predict(X_test_vec)
print(y_predTitle)

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(x_Headline)
X_test_vec = vectorizer.transform(dfTest['clean_headline'])

y_predHeadline = svmHeadline.predict(X_test_vec)
print(y_predHeadline)

print(type(y_predTitle))
print(type(y_predHeadline))

y_idLink = dfTest['IDLink'].to_numpy()
print(type(y_idLink))

submission = pd.DataFrame({
    "IDLink": y_idLink,
    "SentimentTitle": y_predTitle,
    "SentimentHeadline": y_predHeadline
})
submission.to_csv("submission.csv", index=False)

